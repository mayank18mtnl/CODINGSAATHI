{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1w3v9-XQrLyy7ewhvDbSUCsHObIlYLoWK",
      "authorship_tag": "ABX9TyOyzsM4A33EKIJR68drpJTp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayank18mtnl/CODINGSAATHI/blob/main/next_word_prediction_MayankMaiti.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NEXT WORD PREDICTION**\n",
        "# LEVEL 3 TASK 2\n",
        "To predict the next word after the user tries to enter a sentence or a series  of words ."
      ],
      "metadata": {
        "id": "Zvhxq-ZIMzja"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hgpp25JNBeXg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/drive/MyDrive/1661-0.txt\", \"r\", encoding = \"utf8\")\n",
        "\n",
        "# store file in list\n",
        "lines = []\n",
        "for i in file:\n",
        "    lines.append(i)\n",
        "\n",
        "# Convert list to string\n",
        "data = \"\"\n",
        "for i in lines:\n",
        "  data = ' '. join(lines)\n",
        "\n",
        "#replace unnecessary stuff with space\n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')  #new line, carriage return, unicode character --> replace by space\n",
        "\n",
        "#remove unnecessary spaces\n",
        "data = data.split()\n",
        "data = ' '.join(data)\n",
        "data[:500]"
      ],
      "metadata": {
        "id": "L1nDlLKvBrRj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "8db51436-ba1e-446f-fbdd-79a2db25f25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.net Title: The Adventures of Sherlock Holmes Author: Arthur Conan Doyle Release Date: November 29, 2002 [EBook #1661] Last Updated: May 20, 2019 Language: English Character set en\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "5ecKs6IgBrZH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c3c18b-0ee0-4ffc-f59a-7dd153b4091f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "573660"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "# saving the tokenizer for predict function\n",
        "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:15]"
      ],
      "metadata": {
        "id": "fEFqAIZ9BrqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "432cf6a6-f070-4dcf-bac2-747f69c49a9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[142, 4680, 1, 986, 5, 125, 33, 46, 556, 2164, 2165, 27, 987, 14, 22]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sequence_data)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)\n",
        "sequences = []\n",
        "\n",
        "for i in range(3, len(sequence_data)):\n",
        "    words = sequence_data[i-3:i+1]\n",
        "    sequences.append(words)\n",
        "\n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]"
      ],
      "metadata": {
        "id": "vAXifDDBBrx4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "838f048d-aedd-4ea6-9bcb-6398d9ffb541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8624\n",
            "The Length of sequences are:  108955\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 142, 4680,    1,  986],\n",
              "       [4680,    1,  986,    5],\n",
              "       [   1,  986,    5,  125],\n",
              "       [ 986,    5,  125,   33],\n",
              "       [   5,  125,   33,   46],\n",
              "       [ 125,   33,   46,  556],\n",
              "       [  33,   46,  556, 2164],\n",
              "       [  46,  556, 2164, 2165],\n",
              "       [ 556, 2164, 2165,   27],\n",
              "       [2164, 2165,   27,  987]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "    X.append(i[0:3])\n",
        "    y.append(i[3])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "print(\"Data: \", X[:10])\n",
        "print(\"Response: \", y[:10])\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "y[:5]"
      ],
      "metadata": {
        "id": "jjsSYjzNBr6A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b69d3da3-92fa-4567-9f36-c56d70d3eeb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data:  [[ 142 4680    1]\n",
            " [4680    1  986]\n",
            " [   1  986    5]\n",
            " [ 986    5  125]\n",
            " [   5  125   33]\n",
            " [ 125   33   46]\n",
            " [  33   46  556]\n",
            " [  46  556 2164]\n",
            " [ 556 2164 2165]\n",
            " [2164 2165   27]]\n",
            "Response:  [ 986    5  125   33   46  556 2164 2165   27  987]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=3))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "NKjOZDrnBsBB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7b37d99-6c62-42d3-daae-1ac552ede2a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 3, 10)             86240     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 3, 1000)           4044000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8624)              8632624   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,767,864\n",
            "Trainable params: 21,767,864\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"next_words.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))\n",
        "model.fit(X, y, epochs=50, batch_size=64, callbacks=[checkpoint])"
      ],
      "metadata": {
        "id": "ONVje3BvBsHY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a40db018-cb0b-4fe5-9ada-dedd201baadb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 6.4043\n",
            "Epoch 1: loss improved from inf to 6.40432, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 47s 22ms/step - loss: 6.4043\n",
            "Epoch 2/50\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 5.8131\n",
            "Epoch 2: loss improved from 6.40432 to 5.81314, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 5.8131\n",
            "Epoch 3/50\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 5.4882\n",
            "Epoch 3: loss improved from 5.81314 to 5.48816, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 5.4882\n",
            "Epoch 4/50\n",
            "1700/1703 [============================>.] - ETA: 0s - loss: 5.2311\n",
            "Epoch 4: loss improved from 5.48816 to 5.23093, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 5.2309\n",
            "Epoch 5/50\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 5.0004\n",
            "Epoch 5: loss improved from 5.23093 to 5.00041, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 5.0004\n",
            "Epoch 6/50\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 4.7767\n",
            "Epoch 6: loss improved from 5.00041 to 4.77668, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 28s 16ms/step - loss: 4.7767\n",
            "Epoch 7/50\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 4.5495\n",
            "Epoch 7: loss improved from 4.77668 to 4.54952, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 4.5495\n",
            "Epoch 8/50\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 4.3220\n",
            "Epoch 8: loss improved from 4.54952 to 4.32197, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 4.3220\n",
            "Epoch 9/50\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 4.0909\n",
            "Epoch 9: loss improved from 4.32197 to 4.09100, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 4.0910\n",
            "Epoch 10/50\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 3.8497\n",
            "Epoch 10: loss improved from 4.09100 to 3.84953, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 3.8495\n",
            "Epoch 11/50\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 3.6021\n",
            "Epoch 11: loss improved from 3.84953 to 3.60207, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 3.6021\n",
            "Epoch 12/50\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 3.3517\n",
            "Epoch 12: loss improved from 3.60207 to 3.35170, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 3.3517\n",
            "Epoch 13/50\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 3.1120\n",
            "Epoch 13: loss improved from 3.35170 to 3.11198, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 3.1120\n",
            "Epoch 14/50\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 2.8720\n",
            "Epoch 14: loss improved from 3.11198 to 2.87191, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 2.8719\n",
            "Epoch 15/50\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 2.6322\n",
            "Epoch 15: loss improved from 2.87191 to 2.63221, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 2.6322\n",
            "Epoch 16/50\n",
            "1700/1703 [============================>.] - ETA: 0s - loss: 2.3974\n",
            "Epoch 16: loss improved from 2.63221 to 2.39727, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 2.3973\n",
            "Epoch 17/50\n",
            "1700/1703 [============================>.] - ETA: 0s - loss: 2.1652\n",
            "Epoch 17: loss improved from 2.39727 to 2.16556, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 2.1656\n",
            "Epoch 18/50\n",
            "1700/1703 [============================>.] - ETA: 0s - loss: 1.9453\n",
            "Epoch 18: loss improved from 2.16556 to 1.94570, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 1.9457\n",
            "Epoch 19/50\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 1.7390\n",
            "Epoch 19: loss improved from 1.94570 to 1.73898, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 1.7390\n",
            "Epoch 20/50\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 1.5548\n",
            "Epoch 20: loss improved from 1.73898 to 1.55483, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 28s 16ms/step - loss: 1.5548\n",
            "Epoch 21/50\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 1.3838\n",
            "Epoch 21: loss improved from 1.55483 to 1.38396, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 17ms/step - loss: 1.3840\n",
            "Epoch 22/50\n",
            "1700/1703 [============================>.] - ETA: 0s - loss: 1.2422\n",
            "Epoch 22: loss improved from 1.38396 to 1.24223, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 1.2422\n",
            "Epoch 23/50\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 1.1218\n",
            "Epoch 23: loss improved from 1.24223 to 1.12181, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 1.1218\n",
            "Epoch 24/50\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 1.0222\n",
            "Epoch 24: loss improved from 1.12181 to 1.02223, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 1.0222\n",
            "Epoch 25/50\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 0.9428\n",
            "Epoch 25: loss improved from 1.02223 to 0.94280, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 0.9428\n",
            "Epoch 26/50\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 0.8672\n",
            "Epoch 26: loss improved from 0.94280 to 0.86754, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.8675\n",
            "Epoch 27/50\n",
            "1700/1703 [============================>.] - ETA: 0s - loss: 0.8187\n",
            "Epoch 27: loss improved from 0.86754 to 0.81899, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 0.8190\n",
            "Epoch 28/50\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 0.7648\n",
            "Epoch 28: loss improved from 0.81899 to 0.76497, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 0.7650\n",
            "Epoch 29/50\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 0.7299\n",
            "Epoch 29: loss improved from 0.76497 to 0.72986, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 0.7299\n",
            "Epoch 30/50\n",
            "1700/1703 [============================>.] - ETA: 0s - loss: 0.6984\n",
            "Epoch 30: loss improved from 0.72986 to 0.69828, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 0.6983\n",
            "Epoch 31/50\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 0.6721\n",
            "Epoch 31: loss improved from 0.69828 to 0.67231, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 0.6723\n",
            "Epoch 32/50\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 0.6483\n",
            "Epoch 32: loss improved from 0.67231 to 0.64833, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 0.6483\n",
            "Epoch 33/50\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 0.6307\n",
            "Epoch 33: loss improved from 0.64833 to 0.63064, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 0.6306\n",
            "Epoch 34/50\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 0.6094\n",
            "Epoch 34: loss improved from 0.63064 to 0.60944, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 0.6094\n",
            "Epoch 35/50\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 0.5916\n",
            "Epoch 35: loss improved from 0.60944 to 0.59163, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 0.5916\n",
            "Epoch 36/50\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 0.5796\n",
            "Epoch 36: loss improved from 0.59163 to 0.57972, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 0.5797\n",
            "Epoch 37/50\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 0.5678\n",
            "Epoch 37: loss improved from 0.57972 to 0.56781, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 0.5678\n",
            "Epoch 38/50\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 0.5532\n",
            "Epoch 38: loss improved from 0.56781 to 0.55321, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 31s 18ms/step - loss: 0.5532\n",
            "Epoch 39/50\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 0.5462\n",
            "Epoch 39: loss improved from 0.55321 to 0.54615, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 0.5462\n",
            "Epoch 40/50\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 0.5393\n",
            "Epoch 40: loss improved from 0.54615 to 0.53933, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 0.5393\n",
            "Epoch 41/50\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 0.5233\n",
            "Epoch 41: loss improved from 0.53933 to 0.52342, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 0.5234\n",
            "Epoch 42/50\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 0.5150\n",
            "Epoch 42: loss improved from 0.52342 to 0.51496, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 0.5150\n",
            "Epoch 43/50\n",
            "1700/1703 [============================>.] - ETA: 0s - loss: 0.5110\n",
            "Epoch 43: loss improved from 0.51496 to 0.51083, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 0.5108\n",
            "Epoch 44/50\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 0.5017\n",
            "Epoch 44: loss improved from 0.51083 to 0.50171, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 0.5017\n",
            "Epoch 45/50\n",
            "1703/1703 [==============================] - ETA: 0s - loss: 0.4955\n",
            "Epoch 45: loss improved from 0.50171 to 0.49548, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.4955\n",
            "Epoch 46/50\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 0.4909\n",
            "Epoch 46: loss improved from 0.49548 to 0.49088, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.4909\n",
            "Epoch 47/50\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 0.4808\n",
            "Epoch 47: loss improved from 0.49088 to 0.48075, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 30s 18ms/step - loss: 0.4808\n",
            "Epoch 48/50\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 0.4759\n",
            "Epoch 48: loss improved from 0.48075 to 0.47593, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 0.4759\n",
            "Epoch 49/50\n",
            "1702/1703 [============================>.] - ETA: 0s - loss: 0.4713\n",
            "Epoch 49: loss improved from 0.47593 to 0.47133, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 0.4713\n",
            "Epoch 50/50\n",
            "1701/1703 [============================>.] - ETA: 0s - loss: 0.4654\n",
            "Epoch 50: loss improved from 0.47133 to 0.46542, saving model to next_words.h5\n",
            "1703/1703 [==============================] - 27s 16ms/step - loss: 0.4654\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7aac3fe5e590>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = load_model('next_words.h5')\n",
        "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
        "\n",
        "def Predict_Next_Words(model, tokenizer, text):\n",
        "\n",
        "  sequence = tokenizer.texts_to_sequences([text])\n",
        "  sequence = np.array(sequence)\n",
        "  preds = np.argmax(model.predict(sequence))\n",
        "  predicted_word = \"\"\n",
        "\n",
        "  for key, value in tokenizer.word_index.items():\n",
        "      if value == preds:\n",
        "          predicted_word = key\n",
        "          break\n",
        "\n",
        "  print(predicted_word)\n",
        "  return predicted_word"
      ],
      "metadata": {
        "id": "E0ePcpudGnog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while(True):\n",
        "  text = input(\"Enter your line: \")\n",
        "\n",
        "  if text == \"end\":\n",
        "      print(\"Execution completed.....\")\n",
        "      break\n",
        "\n",
        "  else:\n",
        "      try:\n",
        "          text = text.split(\" \")\n",
        "          text = text[-3:]\n",
        "          print(text)\n",
        "\n",
        "          Predict_Next_Words(model, tokenizer, text)\n",
        "\n",
        "      except Exception as e:\n",
        "        print(\"Error occurred: \",e)\n",
        "        continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1iQYw3eGoJU",
        "outputId": "3c621b56-b751-4917-f68c-f6300213b185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your line: you are a \n",
            "['are', 'a', '']\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "man\n",
            "Enter your line: can I call you \n",
            "['call', 'you', '']\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "in\n",
            "Enter your line: know the possibility\n",
            "['know', 'the', 'possibility']\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "of\n",
            "Enter your line: trust the \n",
            "['trust', 'the', '']\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "to\n",
            "Enter your line: whenever you cry remember to \n",
            "['remember', 'to', '']\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "my\n",
            "Enter your line: end\n",
            "Execution completed.....\n"
          ]
        }
      ]
    }
  ]
}